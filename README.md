# AC-GCAT: è§£å‰–çº¦æŸå›¾äº¤å‰æ³¨æ„åŠ›Transformer

è¿™æ˜¯è®ºæ–‡ **"AC-GCAT: åŸºäºè§£å‰–çº¦æŸé—¨æ§Transformerçš„é²æ£’é™æ€æ‰‹åŠ¿è¯†åˆ«"** çš„å®˜æ–¹PyTorchå®ç°ã€‚

![](./figures/framework.jpg)

## ğŸŒŸ äº®ç‚¹

- **SOTAæ€§èƒ½**: åœ¨ASLå­—æ¯æ•°æ®é›†ä¸Šè¾¾åˆ°99.87%çš„å‡†ç¡®ç‡,å…·æœ‰å“è¶Šçš„é²æ£’æ€§(é«˜æ–¯1.0å™ªå£°ä¸‹99.43%)
- **è¶…é«˜æ•ˆ**: ä»…0.0864 GFLOPs(æ¯”ResNet-50ä½48å€),å‚æ•°é‡2.31M
- **å®æ—¶å°±ç»ª**: åœ¨RTX 4090ä¸Šæ¨ç†æ—¶é—´3.05ms,é€‚åˆè¾¹ç¼˜éƒ¨ç½²
- **å¤šæ¨¡æ€èåˆ**: ç»“åˆéª¨æ¶å…³é”®ç‚¹å’Œè§†è§‰ç‰¹å¾çš„æ–°å‹é—¨æ§äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
- **è§£å‰–çº¦æŸ**: ç½®ä¿¡åº¦æ„ŸçŸ¥çš„è½¯çº¦æŸ,ç¡®ä¿ç”Ÿç‰©åŠ›å­¦ä¸Šåˆç†çš„é¢„æµ‹

## ç¯å¢ƒè¦æ±‚

![Python >=3.8](https://img.shields.io/badge/Python->=3.8-yellow.svg)
![PyTorch >=2.0](https://img.shields.io/badge/PyTorch->=2.0-blue.svg)

```bash
torch>=2.0.0
torchvision>=0.15.0
pandas>=1.5.0
numpy>=1.24.0
scikit-learn>=1.2.0
matplotlib>=3.6.0
seaborn>=0.12.0
joblib>=1.2.0
```

## æ•°æ®å‡†å¤‡

### æ”¯æŒçš„æ•°æ®é›†

- **ASLå­—æ¯æ•°æ®é›†** (29ç±»): [Kaggleé“¾æ¥](https://www.kaggle.com/dsv/29550) - 87,000å¼ å›¾åƒ
- **NUSæ‰‹åŠ¿æ•°æ®é›†** (10ç±»): [å®˜æ–¹ç½‘ç«™](https://www.ece.nus.edu.sg/stfpage/elepv/NUS-HandSet/)
- **å°åº¦æ‰‹è¯­æ•°æ®é›†** (36ç±»): [Kaggleé“¾æ¥](https://www.kaggle.com/datasets/kartik2112/indian-sign-language-translation-letters-n-digits)
- **ASLæ•°æ®é›†(å°å‹)** (9ç±»): [Kaggleé“¾æ¥](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)

### æ•°æ®å¤„ç†æµç¨‹

#### æ­¥éª¤1: æå–æ‰‹éƒ¨å…³é”®ç‚¹ (MediaPipe)

```python
import cv2
import mediapipe as mp
import pandas as pd

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)

data = []
for img_path in image_paths:
    image = cv2.imread(img_path)
    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    
    if results.multi_hand_landmarks:
        landmarks = results.multi_hand_landmarks[0]
        row = {'image_path': img_path, 'label': label}
        
        for idx, lm in enumerate(landmarks.landmark):
            row[f'x_{idx}'] = lm.x
            row[f'y_{idx}'] = lm.y
            row[f'z_{idx}'] = lm.z
        
        data.append(row)

df = pd.DataFrame(data)
df.to_csv('all_landmarks_multimodal.csv', index=False)
```

#### æ­¥éª¤2: æå–å›¾åƒç‰¹å¾ (ResNet-50)

```python
import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import numpy as np

# åŠ è½½é¢„è®­ç»ƒçš„ResNet50
model = models.resnet50(pretrained=True)
model = torch.nn.Sequential(*list(model.children())[:-1])  # ç§»é™¤åˆ†ç±»å™¨
model.eval()

# å›¾åƒé¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                        std=[0.229, 0.224, 0.225])
])

# æå–ç‰¹å¾
features = []
for img_path in df['image_path']:
    img = Image.open(img_path).convert('RGB')
    img_tensor = transform(img).unsqueeze(0)
    
    with torch.no_grad():
        feature = model(img_tensor).squeeze().numpy()
    features.append(feature)

# ä¿å­˜ç‰¹å¾
np.save('all_image_features_resnet50.npy', np.array(features))
```

### ä½¿ç”¨é¢„å¤„ç†æ•°æ®å¿«é€Ÿå¼€å§‹

é¢„æœŸçš„æ–‡ä»¶ç»“æ„:
```
data/
â”œâ”€â”€ ASL_Alphabet/
â”‚   â”œâ”€â”€ all_landmarks_multimodal.csv
â”‚   â””â”€â”€ all_image_features_resnet50.npy
â”œâ”€â”€ NUS_Hand_Posture/
â”‚   â”œâ”€â”€ all_landmarks_multimodal.csv
â”‚   â””â”€â”€ all_image_features_resnet50.npy
â”œâ”€â”€ Indian_Sign_Language/
â”‚   â”œâ”€â”€ all_landmarks_multimodal.csv
â”‚   â””â”€â”€ all_image_features_resnet50.npy
â””â”€â”€ asl_dataset/
    â”œâ”€â”€ all_landmarks_multimodal.csv
    â””â”€â”€ all_image_features_resnet50.npy
```

## å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourusername/AC-GCAT.git
cd AC-GCAT

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windowsç³»ç»Ÿ: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install pandas numpy scikit-learn matplotlib seaborn joblib
```

## è®­ç»ƒ

### è®­ç»ƒå®Œæ•´SOTAæ¨¡å‹(æ¨è)

```bash
python main.py \
    --study_name sota_only \
    --epochs 50 \
    --batch_size 128 \
    --learning_rate 3e-4 \
    --train_csv_path "data/ASL_Alphabet/all_landmarks_multimodal.csv" \
    --image_features_path "data/ASL_Alphabet/all_image_features_resnet50.npy" \
    --output_dir "./results_asl"
```

### è¿è¡Œå®Œæ•´æ¶ˆèç ”ç©¶(15ä¸ªå®éªŒ)

```bash
python main.py \
    --study_name full_ablation \
    --epochs 50 \
    --train_csv_path "data/ASL_Alphabet/all_landmarks_multimodal.csv" \
    --image_features_path "data/ASL_Alphabet/all_image_features_resnet50.npy" \
    --output_dir "./results_ablation"
```

è¿™å°†è¿è¡Œ:
- **10ä¸ªçº¿æ€§å®éªŒ**: æ¸è¿›å¼ç»„ä»¶æ·»åŠ (å®éªŒ1-10)
- **5ä¸ªæ–°ç»„åˆå®éªŒ**: æ›¿ä»£é…ç½®(å®éªŒ11-15)

### è¶…å‚æ•°æœç´¢

```bash
# åµŒå…¥ç»´åº¦æœç´¢ [128, 256, 384, 512]
python main.py --study_name hyperparam_embed_dim --epochs 50

# å±‚æ•°æœç´¢ [2, 4, 6, 8]
python main.py --study_name hyperparam_num_layers --epochs 50

# æ³¨æ„åŠ›å¤´æ•°æœç´¢ [4, 8, 16]
python main.py --study_name hyperparam_num_heads --epochs 50

# å¯¹æ¯”æŸå¤±æ¸©åº¦æœç´¢ [0.05, 0.07, 0.1, 0.2, 0.5]
python main.py --study_name hyperparam_cont_temp --epochs 50

# è§£å‰–æŸå¤±æƒé‡æœç´¢ [0.0, 0.01, 0.02, 0.05, 0.1]
python main.py --study_name hyperparam_anat --epochs 50

# å¯¹æ¯”æŸå¤±æƒé‡æœç´¢ [0.0, 0.05, 0.1, 0.15, 0.2]
python main.py --study_name hyperparam_contrastive --epochs 50
```

### æ‰¹é‡å¤„ç†å¤šä¸ªæ•°æ®é›†

```bash
chmod +x run_all_datasets.sh
./run_all_datasets.sh
```

ç¼–è¾‘ `run_all_datasets.sh` ä»¥è‡ªå®šä¹‰æ•°æ®é›†è·¯å¾„ã€‚

## æµ‹è¯•

### æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹

```bash
python main.py \
    --mode test \
    --model_path "./results/weights/best_model_Exp_10_Full_SOTA_21kpts.pth" \
    --model_name "HybridAttentionModel" \
    --test_csv_path "data/test_set/test_landmarks.csv" \
    --image_features_path "data/test_set/test_image_features.npy" \
    --output_dir "./results"
```

## æ¨¡å‹æ¶æ„

### å…³é”®ç»„ä»¶

1. **ç‰¹å¾å·¥ç¨‹(C1)**: å¢å¼º3Då…³é”®ç‚¹(x, y, z),åŒ…æ‹¬:
   - éª¨éª¼å‘é‡(3ç»´)
   - å…³èŠ‚è§’åº¦(1ç»´)
   - æŒ‡å°–è·ç¦»(4ç»´)
   - æ€»è®¡: æ¯ä¸ªå…³é”®ç‚¹11ç»´

2. **å¤šæ¨¡æ€èåˆ(C2)**:
   - **è‡ªæ³¨æ„åŠ›**: 4å±‚Transformerç¼–ç å™¨å¤„ç†éª¨æ¶ç‰¹å¾
   - **äº¤å‰æ³¨æ„åŠ›**: éª¨æ¶ç‰¹å¾(Query) Ã— å›¾åƒç‰¹å¾(Key/Value)
   - **è‡ªé€‚åº”å…¨å±€é—¨æ§**: èåˆåçš„åŠ¨æ€ç‰¹å¾ä¼˜åŒ–

3. **åŒé‡çº¦æŸæŸå¤±(C3)**:
   - **äº¤å‰ç†µæŸå¤±**: ä½¿ç”¨æ ‡ç­¾å¹³æ»‘(0.1)
   - **è§£å‰–æŸå¤±**: ç½®ä¿¡åº¦æ„ŸçŸ¥çš„è½¯çº¦æŸ
     - å…³èŠ‚è§’åº¦çº¦æŸ(æ”¾å®½è‡³[-Ï€/2, 3Ï€/2])
     - éª¨éª¼é•¿åº¦æ¯”éªŒè¯(æ”¾å®½è‡³[0.5-1.5Ã—, 0.4-1.2Ã—])
     - æ‰‹æŒå¹³é¢æ€§æ­£åˆ™åŒ–
     - ä½™å¼¦é€€ç«è°ƒåº¦(å¼ºåˆ°å¼±)
   - **ç›‘ç£å¯¹æ¯”æŸå¤±(InfoNCE)**: æ¸©åº¦Ï„=0.1

### æ¶æ„é…ç½®

| æ¨¡å‹ | è¾“å…¥ç»´åº¦ | åµŒå…¥ç»´åº¦ | å±‚æ•° | æ³¨æ„åŠ›å¤´ | å‚æ•°é‡(M) | FLOPs(G) |
|------|----------|----------|------|----------|-----------|----------|
| MLPBaseline | 3D/11D | - | - | - | 2.1 | 0.0042 |
| AdvancedAttentionModel | 11D | 256 | 4 | 8 | 8.5 | 0.0312 |
| ImageOnlyBaseline | 2048D | - | - | - | 25.6 | 0.0026 |
| **HybridAttentionModel (SOTA)** | **11D+2048D** | **256** | **4** | **8** | **19.10** | **0.0864** |

## å®éªŒç»“æœ

### ASLå­—æ¯æ•°æ®é›†æ€§èƒ½(87kå›¾åƒ, 29ç±»)

| é…ç½® | æµ‹è¯•å‡†ç¡®ç‡ | é«˜æ–¯0.5 | é«˜æ–¯1.0 | é®æŒ¡3ç‚¹ | ç»“æ„åŒ–å™ªå£° |
|------|-----------|---------|---------|---------|-----------|
| å®éªŒ1 (MLPåŸºç¡€) | 97.97% | 82.07% | 53.84% | 88.15% | 89.60% |
| å®éªŒ3 (æ³¨æ„åŠ›é«˜çº§) | 99.38% | 97.71% | 91.33% | 99.34% | 98.66% |
| å®éªŒ5 (åæœŸèåˆ) | 99.65% | 99.32% | 97.59% | 99.63% | 99.41% |
| å®éªŒ7 (é—¨æ§äº¤å‰æ³¨æ„åŠ›) | 99.79% | 99.74% | 99.57% | 99.79% | 99.77% |
| **å®éªŒ10 (å®Œæ•´SOTA)** | **99.87%** | **99.73%** | **99.43%** | **99.79%** | **99.80%** |

### è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›

| æ•°æ®é›† | è§„æ¨¡ | ç±»åˆ«æ•° | æµ‹è¯•å‡†ç¡®ç‡ | é«˜æ–¯1.0 |
|--------|------|--------|-----------|---------|
| NUSæ‰‹åŠ¿ | 422 | 10 | 100.00% | 97.65% |
| å°åº¦æ‰‹è¯­ | 9,023 | 36 | 99.61% | 99.56% |
| ASLæ•°æ®é›†(å°å‹) | 1,819 | 9 | 92.86% | **92.03%** âš ï¸ |
| ASLå­—æ¯ | 87,000 | 29 | 99.87% | **99.49%** âœ… |

âš ï¸ **å…³é”®å‘ç°**: é²æ£’æ€§é«˜åº¦ä¾èµ–äºå¤§è§„æ¨¡è®­ç»ƒæ•°æ®ã€‚

### ä¸SOTAæ–¹æ³•å¯¹æ¯”

| æ¨¡å‹ | æ¶æ„ | å‡†ç¡®ç‡ | å‚æ•°é‡(M) | FLOPs(G) | æ¨ç†æ—¶é—´(ms) |
|------|------|--------|-----------|----------|-------------|
| ResNet-50 | 50å±‚æ®‹å·®ç½‘ç»œ | 97.41% | 25.6 | 4.1 | 15-20 |
| Vision Transformer | 12å±‚ViT-Base | 88.59% | 86.6 | 17.6 | 25-35 |
| 4å±‚CNN | Conv-Pool-Conv-Pool-FC | 99.91% | <5 | <1 | 3-5 |
| **AC-GCAT(æˆ‘ä»¬çš„)** | **é—¨æ§äº¤å‰æ³¨æ„åŠ›** | **99.87%** | **19.10** | **0.0864** | **3.05** |

## å…³é”®è¶…å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--embed_dim` | 256 | åµŒå…¥ç»´åº¦(å¿…é¡»èƒ½è¢«num_headsæ•´é™¤) |
| `--num_layers` | 4 | Transformerå—æ•°é‡ |
| `--num_heads` | 8 | æ³¨æ„åŠ›å¤´æ•°é‡ |
| `--dropout` | 0.2 | Dropoutç‡ |
| `--batch_size` | 128 | è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| `--learning_rate` | 3e-4 | OneCycleLRæœ€å¤§å­¦ä¹ ç‡ |
| `--weight_decay` | 0.05 | AdamWæƒé‡è¡°å‡ |
| `--anatomical_weight_final` | 0.02 | è§£å‰–æŸå¤±æœ€ç»ˆæƒé‡ |
| `--contrastive_weight` | 0.1 | å¯¹æ¯”æŸå¤±æƒé‡ |
| `--contrastive_temperature` | 0.1 | InfoNCEæŸå¤±æ¸©åº¦ |
| `--early_stopping_patience` | 10 | æ—©åœè€å¿ƒå€¼(0=ç¦ç”¨) |
| `--seed` | 42 | éšæœºç§å­,ä¿è¯å¯å¤ç°æ€§ |

## è¾“å‡ºæ–‡ä»¶

è®­ç»ƒå,æ¡†æ¶ä¼šç”Ÿæˆ:

1. **æ€»ç»“æŠ¥å‘Š**: `summary_report_<study_name>_<timestamp>.txt`
   - æ‰€æœ‰å®éªŒçš„å‡†ç¡®ç‡æ’å
   - é²æ£’æ€§è¯„åˆ†(é«˜æ–¯1.0)
   - è®­ç»ƒæ—¶é—´å¯¹æ¯”

2. **æ¨¡å‹æƒé‡**: `weights/best_model_<exp_name>_21kpts.pth`

3. **è®­ç»ƒæ›²çº¿å›¾**: `plots/history_<exp_name>.png`
   - è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿
   - éªŒè¯å‡†ç¡®ç‡æ›²çº¿

4. **æ··æ·†çŸ©é˜µ**: `confusion_matrix/cm_<model_name>.png`

5. **é¢„å¤„ç†å·¥å…·**: 
   - `scaler_21kpts.pkl`
   - `label_encoder_21kpts.pkl`

6. **é²æ£’æ€§æµ‹è¯•ç»“æœ**: åŒ…å«ä»¥ä¸‹å™ªå£°æµ‹è¯•:
   - é«˜æ–¯å™ªå£°(Ïƒ=0.5, 1.0)
   - éšæœºé®æŒ¡(3ä¸ªå…³é”®ç‚¹)
   - ç»“æ„åŒ–æ‰‹æŒ‡å™ªå£°(Ïƒ=0.8)

## é¡¹ç›®ç»“æ„

```
AC-GCAT/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt             # Pythonä¾èµ–
â”œâ”€â”€ config.py                    # é…ç½®å’Œå‚æ•°è§£æå™¨
â”œâ”€â”€ main.py                      # ä¸»è®­ç»ƒ/æµ‹è¯•åè°ƒå™¨
â”œâ”€â”€ model.py                     # æ¨¡å‹æ¶æ„
â”‚   â”œâ”€â”€ MLPBaseline
â”‚   â”œâ”€â”€ AdvancedAttentionModel
â”‚   â”œâ”€â”€ ImageOnlyBaseline
â”‚   â””â”€â”€ HybridAttentionModel    # SOTAå¤šæ¨¡æ€æ¨¡å‹
â”œâ”€â”€ losses.py                    # è‡ªå®šä¹‰æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ HierarchicalContrastiveLoss (InfoNCE)
â”‚   â””â”€â”€ AnatomicalLoss (ç½®ä¿¡åº¦æ„ŸçŸ¥)
â”œâ”€â”€ train.py                     # è®­ç»ƒå’Œè¯„ä¼°é€»è¾‘
â”œâ”€â”€ utils.py                     # æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
â”œâ”€â”€ run_all_datasets.sh          # å¤šæ•°æ®é›†æ‰¹å¤„ç†è„šæœ¬
â”œâ”€â”€ figures/                     # è®ºæ–‡å›¾è¡¨
â””â”€â”€ data/                        # æ•°æ®ç›®å½•(ä¸åŒ…å«)
    â”œâ”€â”€ ASL_Alphabet/
    â”œâ”€â”€ NUS_Hand_Posture/
    â”œâ”€â”€ Indian_Sign_Language/
    â””â”€â”€ asl_dataset/
```

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨æ­¤ä»£ç ,è¯·å¼•ç”¨:

```bibtex
@article{qu2025acgcat,
  title={AC-GCAT: An Anatomically-Constrained Gated Transformer for Robust Static Hand Gesture Recognition},
  author={Qu, Xianping and Guo, Zhipeng and Su, Xiaobo and Zhou, Caixia},
  journal={Pattern Recognition},
  year={2025}
}
```

## è‡´è°¢

æˆ‘ä»¬çš„ä»£ç æ¡†æ¶å—ä»¥ä¸‹é¡¹ç›®å¯å‘:
- [AimCLR](https://github.com/LinguoLi/AimCLR) ç”¨äºåŸºäºéª¨æ¶çš„å¯¹æ¯”å­¦ä¹ 
- [CrosSCLR](https://github.com/LinguoLi/CrosSCLR) ç”¨äºè·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ 
- MediaPipe ç”¨äºæ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## è”ç³»æ–¹å¼

- **ä½œè€…**: æ›²çŒ®å¹³, éƒ­å¿—é¹
- **é‚®ç®±**: qxp@cqvie.edu.cn, guo.zp@outlook.com
- **å•ä½**: é‡åº†å·¥ç¨‹èŒä¸šæŠ€æœ¯å­¦é™¢

